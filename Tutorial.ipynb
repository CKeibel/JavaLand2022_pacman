{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIlX3bscb_8H"
   },
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9xfV_VJakaI"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxwx0hgyS9l_"
   },
   "source": [
    "# **Auswahl des Spiels**\n",
    "\n",
    "[Hier](https://gym.openai.com/envs/#atari) ist eine vollständige Liste der verfügbaren Spiele zu finden. Um ein Environment zu erstellen muss der vollständige Name des Spiels als String übergeben werden.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "```python\n",
    "game = \"MsPacman-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW8iwvvZUBxy"
   },
   "outputs": [],
   "source": [
    "# Hier kann das Spiel übergeben werden\n",
    "game = \"SpaceInvaders-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Schritt ist notwendig, wenn das Notebook in Google Colab ausgeführt wird. Je nach installierter Version von OpenAIs Gym Bibliothek müssen die Atari-Spiele importiert werden, damit sie in Gym verfürbar sind. Die Spiele können [Hier](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html) heruntergeladen werden und müssen anschließend in Google Drive hinterlegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    env = gym.make(game)\n",
    "except:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !python -m atari_py.import_roms \"drive/MyDrive/Atari_Roms\"\n",
    "    env = gym.make(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkVT49we0eiY"
   },
   "source": [
    "# **Preprocessing**\n",
    "Das Preprocessing der Daten ist enorm wichtig, da es einen großen Einfluss auf den Lernprozess des Agenten am Ende des Tages hat.\n",
    "Die Bibliothek Gym stellt hierfür Wrapper bereit, die die verschiedenen Spiele um Funktionalitäten erweitern können und das Lernen optimieren.\n",
    "\n",
    "Unter [Stable Baselines](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py) sind schon vordefinierte Wrapper implementiert, die zum Training des Agenten nützlich sind. Die von \"Stable Baselines\" übernommenen Wrapper werden im folgenden kurz erläutert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oHRVfJL9TxH"
   },
   "source": [
    "### Fire Wrapper\n",
    "Der FireResetEnv Wrapper ist dafür zuständig, dass die \"FIRE\"-Aktion bei Spielreset ausgeführt wird. In manchen Spielen ist dies notwendig, damit das Spiel startet. Der Wrapper fragt zuvor die gültigen Aktionen ab und führt \"FIRE\" nur aus, wenn diese sich im Aktionsraum befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxYChLgl0aQu"
   },
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env) \n",
    "        self.env.reset()\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        observation, _, _, _ = self.env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAndSkip Wrapper\n",
    "Aufeinanderfolgende Zustände in Atari-Spielen können schwer voneinander zu unterscheiden sein, da diese in kurzen Zeitabständen aufeinander folgen. Die Änderungen können so klein sein, dass diese für das Verhalten des Agenten nicht relevant erscheinen und deswegen übersprungen werden können. Der MaxAndSkipEnv Wrapper führt für N aufeinanderfolgende Aktionen die gleiche Aktion aus, da die Änderungen in diesem Zeitraum zu klein erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledFloatFrame Wrapper\n",
    "Künstliche neuronale Netze lernen effektiver, wenn sich die eingegbenen Werte in einem Intervall von 0 bis 1 oder -1 bis 1 befinden. Der ScaledFloatFrame Wrapper skalliert die Bilder auf ein vom Benutzer festgelegtes Intervall herunter. Das Intervall kann mit den Parametern \"low\" und \"high\" festgelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpisodicLifeEnv Wrapper\n",
    "Manche Atari-Spiele lassen es zu, dass der Agent nach einer gescheiterten Aktion von einem Checkpoint aus startet. Das ermöglicht es ihm auch fortgeschrittene Zustände zu betreten und zu sampeln. Es kann aber auch vorkommen, dass er fehlerhafte Aktionen mit in das Lernen aufnimmt, da er so in fortgeschrittene Zustände kommt. Bei dem Spiel Breakout-v0 kommt es häufig vor, dass der Agent die ersten Bälle durchlässt, bevor er agiert. Der Agent positioniert sich dabei an einen Spielfeldrand, da der Spielball ab einen bestimmten Zeipunkt automatisch dort abgeworfen wird. Erst dann beginnt der Agent richtig zu spielen.\n",
    "Dieses Verhalten ist aber nicht immer erwünscht, deshalb kann der EpisodicLifeEnv Wrapper eingesetzt werden. Dieser Wrapper verhindert das Starten aus Checkpoints heraus und startet das Spiel neu sobald der Agent eine fehlerhafte Aktion begangen hat.\n",
    "Dies kann aber auch zum Nachteil haben, dass der Agent erst spät fortgeschrittene Spielzustände betritt und das Training zum angestrebten Leistungsziel verlängert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip Reward Wrapper\n",
    "Der ClipRewardEnv Wrapper beschneidet den Reward auf das Intervall von -1 bis 1. Dieses Verhalten kann erwünscht sein, wenn Rewards nicht zu stark von der Umgebung bewertet werden und der Agent vollständig aus eigenen Erfahrungen lernt die Zustände zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULrSsx2f6-qS"
   },
   "source": [
    "### Resize & Grayscale Wrapper\n",
    "Der WarpFrame Wrapper nimmt Einfluss auf die Beobachtung, die der Agent von der Umgebung erhält.\n",
    "Die Ausgangsbilder werden auf Bilder der Größe 84x84 Pixel herunterskaliert und anschleßend auf einen Farbkanal reduziert. Die Farben sind für das Lernen des Spiels nicht wichtig, sodass diese im Sinne einer besseren Rechenkapazität ausgelassen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXr6gB0F7AwT"
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Codezelle ist optional ausführbar und instanziirt eine Umgebung beispielhaft, um den Effekt des WarpFrame Wrapper darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "_KJgqyxF7FRx",
    "outputId": "66a13e33-38f1-4b8b-9156-0f46ca7b3c35"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "normal_env = gym.make(game)\n",
    "wrapped_env = WarpFrameEnv(game)\n",
    "\n",
    "normal_env.reset()\n",
    "wrapped_env.reset()\n",
    "action = normal_env.action_space.sample()\n",
    "\n",
    "normal_state, _, _, _ = normal_env.step(action)\n",
    "wrapped_state, _, _, _ = wrapped_env.step(action)\n",
    "\n",
    "wrapped_state = wrapped_state[: , :, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Warp Frame', fontsize=20)\n",
    "axs[0].imshow(normal_state)\n",
    "axs[0].set_title(\"Normal\", fontsize=16)\n",
    "axs[1].imshow(wrapped_state, cmap=\"gray\")\n",
    "axs[1].set_title(\"Warp Frame\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6m9sMMgAIL_"
   },
   "source": [
    "### Frame Stack Wrapper\n",
    "Der FrameStack Wrapper stapelt N aufeinanderfolgende Zustände aufeinander, sodass diese dem Agenten zusammen als ein Zustand übergeben werden können.\n",
    "Das Stapeln der Zustände ist hilfreich, da der Agent so Bewegungen in der Umgebung wahrnehmen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9odKR1WDAJP9"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], repeats=4, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], repeats=4, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(4):\n",
    "            self.frames.append(obs)\n",
    "        frame_stack = np.asarray(self.frames, dtype=np.float32)\n",
    "        frame_stack = np.moveaxis(frame_stack, 0, -1).reshape(1, 84, 84, -1)\n",
    "        return frame_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Codezelle ist optional ausführbar und instanziirt eine Umgebung beispielhaft, um den Effekt des FrameStack Wrapper darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "QcNdmvOnAVvb",
    "outputId": "e14a2bbf-ff06-4397-f7e0-fdbf244494e1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[OPTIONAL]\n",
    "\n",
    "Diese Zelle ist optional ausführbar und dient zur Visualisierung des Wrappers.\n",
    "Die Zelle hat keinen Einfluss auf den Agenten\n",
    "\"\"\"\n",
    "\n",
    "def FrameStackEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = FrameStackEnv(game)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1, 5):\n",
    "  # Führe eine zufällige Aktion aus\n",
    "  state, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Stack umformen, damit das Plotten der vier Bilder gelingt\n",
    "state = state.reshape(84, 84,4)\n",
    "\n",
    "# Frame Stack plotten\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 5))\n",
    "fig.suptitle('Frame Stack', fontsize=20)\n",
    "for i in range(state.shape[2]):\n",
    "    axs[i].imshow(state[:, :, i], cmap=\"gray\")\n",
    "    axs[i].set_title(\"Frame \"+str(i+1), fontsize=16)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg-MI4P9AadK"
   },
   "source": [
    "### Erstellen des Environments\n",
    "Hier werden die gewählten Wrapper an die Umgebung gebunden und diese anschließen instanziiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eSU0VDOAcML"
   },
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = FrameStack(env)\n",
    "    return env\n",
    "\n",
    "env = make_env(game)\n",
    "\n",
    "\n",
    "\"\"\" saving the properties for csv \"\"\"\n",
    "MODE = \"NoEpisodicLife_NoClipReward_PRETRAINED_w_EpisodicLife\"\n",
    "PATH = \"WEIGHTS/\" + game + \"/\" + MODE + \"/\"\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8HSZBWkHkbJ"
   },
   "source": [
    "# **Deep Q-Network und Target Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeLvxPIvLxE8"
   },
   "source": [
    "Hier wird die Kernkomponente des Agenten definiert, also das künstliche neuronale Netz. Zu Beginn speichern wir uns Parameter ab, die bei dem Erstellen des Netzes wichtig sind. \n",
    "\n",
    "- Der Parameter \"INPUT_SHAPE\" speichert die Dimension der Eingabe, also dem von der Umgebung erhaltenen Bild. Die Vier am Ende steht dafür, dass wir vier Bilder aufeinandergestapelt eingeben. Die Dimension der Eingabe hängt von den definierten Parametern im FrameStack Wrapper und WarpFrame Wrapper ab, da diese die Bildeingabe umformen.\n",
    "\n",
    "- Mit OUTPUT_SHAPE legen wir die Anzahl der Neuronen in der Ausgabeschicht fest. Das neuronale Netz braucht ein künstliches Neuron für jede gültige Aktion, die das Spiel zulässt. Mit env.action_space.n können wir die gültigen Aktionen der Umgebungsinstanz abfragen.\n",
    "\n",
    "- Die LOSS_FUNCTION berechnet den Fehler zwischen der Ausgabe des neuronalen Netzes und dem Zielwert, den das neuronale Netz für eine Eingabe erlernen soll.\n",
    "\n",
    "- Der OPITIMIZER ist das angewandte Verfahren, dass den Gradientenabsteig des neuronalen Netzes berechnet.\n",
    "\n",
    "Im nächsten Schritt wird das neuronale Netz bausteinartig zusammengesetzt. Mit net_input definieren wir die Eingabedimension, die die Eingabeschicht annehmen soll. Conv2D sind Faltungsschichten, die sich besonders gut dafür eignen Bilder zu verarbeiten.\n",
    "- Der Parameter filters gibt an, wie viele Filter in der Faltungsschicht zur Verarbeitung der Daten genutzt werden sollen.\n",
    "- kernel_size bestimmt die Größe der eingesetzten Filter.\n",
    "- strides beschreibt die Schrittweite, die ein Filter über die Eingabe verschoben wird.\n",
    "- padding legt fest, ob die ursprüngleiche Dimensionalität der Eingaben beibehalten werden soll.\n",
    "\n",
    "Nach jedem instanzierten Schichtaufruf wird die Eingabe der Schicht angegben. Beispielsweise werden unsere Schichten der Variable x zugewiesen und der nächsten Schicht übergeben. Da immer erst der linke Teil eines Gleicheheitszeichen aufgerufen wird, müssen wir nicht für jede unserer Schichten eine neue Variable definieren.\n",
    "\n",
    "Nach jeder Schicht wird mit einer Aktivierungsfunktion die Aktivierung der Neuronen in einer Schicht berechnet. Die gängigste Aktivierungsfuntion ist dabei die ReLU-Aktivierungsfunktion.\n",
    "\n",
    "Mit dem Aufruf von Flatten werden die berechneten Daten in ein Vektorformat gebracht, sodass sie von einer Dense-Schicht weiterverarbeitet werden können. Dense Schichten sind die \"einfachsten\" Schichten eines neuronalen Netzes und sind in der Literatur unter Feedforward-Schicht zu finden. Der Parameter units gibt an, wie viele künstliche Neuronen sich in einer Dense-Schicht befinden sollen.\n",
    "\n",
    "Zum Schluss instanziieren wir das neuronale Netz mit Model() und geben dabei die Eingabe- und die Ausgabeschicht an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IinnQRAzHhcb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# DQN und Tagret Net Parameters\n",
    "INPUT_SHAPE = (84, 84, 4) # (Höhe, Breite, Frames)\n",
    "OUTPUT_SHAPE = env.action_space.n # Anzahl der möglichen Aktionen\n",
    "LOSS_FUNCTION = \"mean_squared_error\" # Fehlerfunktion\n",
    "OPTIMIZER = Adam(lr=0.00005)\n",
    "\n",
    "# Funktion zum Erstellen eines neuronalen Netzes\n",
    "def build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER):\n",
    "    net_input = Input(shape=INPUT_SHAPE)\n",
    "    x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=512)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    net_output = Dense(OUTPUT_SHAPE)(x)\n",
    "\n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Deep Q-Network\n",
    "DQN = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)\n",
    "# Target Network\n",
    "TARGET = build_neural_net(INPUT_SHAPE, OUTPUT_SHAPE, LOSS_FUNCTION, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VobCYZ0N45l",
    "outputId": "2d95fd9f-17c4-4b27-cdee-18e9109dc8dc"
   },
   "outputs": [],
   "source": [
    "#DQN.summary()\n",
    "#TARGET.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGwUt_ERhMuQ"
   },
   "source": [
    "# **Memory Buffer**\n",
    "Im Memory-Buffer speichern wir zu jedem durchlaufenen Spielzustand die aktuelle Beobachtung (state), die gewählte Aktion (aktion), den erhaltenen Reward (reward), die nachfolgende Beobachtung (next_state) und die done Flag, die signalisiert, ob das Spiel sich in einem Endzustand befindet.\n",
    "\n",
    "Die gespeicherten Tupel werden in \"replay()\" dafür genutzt, um das neuronale Netz zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z601UsvHjzyz"
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 25000\n",
    "MEMORY_BUFFER = deque(maxlen=MEMORY_SIZE)\n",
    "TRAIN_START = 32\n",
    "\n",
    "# Speichern von Transitionen\n",
    "def save_transition(state, action, reward, next_state, done):\n",
    "    MEMORY_BUFFER.append([state, action, reward, next_state, done])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQZvvDESM0QB"
   },
   "source": [
    "# **Experience Replay**\n",
    "In diesem Abschnitt wird das neuronale Netz mit den zuvor gespeicherten Daten trainiert.\n",
    "\n",
    "Es werden MINIBATCH_SIZE viele Tupel aus unserem MEMORY_BUFFER gezogen und in die einzelnen Elemente ihren \"Klassen\" zugeordnet.\n",
    "\n",
    "Durch die neuronalen Netze werden jeweils zu den aktuellen Beobachtungen (states) und den Folgebeobachtungen (next_states) die Q-Values berechnet.\n",
    "\n",
    "Anschließend werden die Q-Values aktualisiert und dem Deep Q-Network als Zielwert zum Training übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9v5SDTFMoZM2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "MINIBATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "\n",
    "def replay():\n",
    "\n",
    "    \"\"\" Ziehe 32 (MINIBATCH_SIZE) zufällige Transitionen aus dem Buffer in einen Minibatch \"\"\"\n",
    "    minibatch = random.sample(MEMORY_BUFFER, MINIBATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "\n",
    "    \"\"\" Vorhersagen der Q-Values \"\"\"\n",
    "    q_values = DQN.predict(states)\n",
    "    q_values_next = TARGET.predict(next_states)\n",
    "\n",
    "    \"\"\" Q-Values Update \"\"\"\n",
    "    for i in range(MINIBATCH_SIZE):\n",
    "        a = actions[i]\n",
    "        done = dones[i]\n",
    "        if done:\n",
    "            q_values[i][a] = rewards[i]\n",
    "        else:\n",
    "            q_values[i][a] = rewards[i] + GAMMA * np.max(q_values_next[i])\n",
    "\n",
    "    \"\"\" Training des neuronalen Netzes \"\"\"\n",
    "    DQN.fit(states, q_values, batch_size=MINIBATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2JdsqGQf_wn"
   },
   "source": [
    "# **Aktion wählen**\n",
    "Das wählen einer Aktion erfolgt nach einer Epsilon-Greedy Strategie. So wird sichergestellt, dass zu Beginn möglichst viele Zustände durchlaufen werden und durch das neuronale Netz bewertet werden können. Zu einen späteren Zeitpunkt wählt der Algorithmus überwiegend gute Aktionen, um eine gute Performance zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineares Verringern von Epsilon\n",
    "def linear(epsilon, decay, epsilon_min, decay_step):\n",
    "    return epsilon - decay\n",
    "\n",
    "# exponentielles Verringern von Epsilon\n",
    "def exponential(epsilon, decay, epsilon_min, decay_step):\n",
    "    epsilon = 1\n",
    "    return epsilon_min + (epsilon - epsilon_min) * np.exp(-decay * decay_step)\n",
    "\n",
    "# gibt die gewählte Verringerungsstrategie zurück\n",
    "def epsilon_decay(mode, epsilon, decay, epsilon_min, decay_step):\n",
    "    \n",
    "    \"\"\" dictionary, in dem die unterschiedlichen Strategien hinterlegt sind \"\"\"\n",
    "    decay_strategy = {\n",
    "        \"linear\": linear,\n",
    "        \"exponential\": exponential\n",
    "    }\n",
    "    \n",
    "    \"\"\" instanziieren einer Strategie \"\"\"\n",
    "    strategy = decay_strategy.get(mode)\n",
    "    return strategy(epsilon, decay, epsilon_min, decay_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktion auf Basis der Explorationsstrategie wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH18a8XDf5ui"
   },
   "outputs": [],
   "source": [
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 0.00002\n",
    "\n",
    "# Wahl einer Aktion\n",
    "def get_action(state):\n",
    "    \n",
    "    \"\"\" ziehe eine zufällige Zahl und vergleiche mit dem aktuellen EPSILON\"\"\"\n",
    "    if np.random.rand() <= EPSILON:\n",
    "        \"\"\" Ist die Zahl kleiner als EPSILON, wird eine zufällige Aktion ausgeführt \"\"\"\n",
    "        return np.random.randint(env.action_space.n)\n",
    "    else:\n",
    "        \"\"\" Ist die Zahl größer als das aktuelle EPSILON, berechnet das neuronale Netz eine Aktion \"\"\"\n",
    "        return np.argmax(DQN(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training des Agenten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Speichern eines initialen Mean Rewards. Während des Trainings wird der Mean Reward aus den \n",
    "letzten zehn gespielten Episoden berechnet. Ist der berechnete Mean Reward besser, so werden die\n",
    "Netzparameter gespeichert und der Mean Reward mit dem aktuellen überschrieben.\n",
    "\n",
    "Der initiale Mean Reward kann nicht auf 0 gesetzt werden, da der Score des Spiels Pong bei -21 startet.\n",
    "Durch Spielen einer zufälligenen Episode samplen wir einen beispielhaften, aber schlechten Reward zu Beginn.\n",
    "\"\"\"\n",
    "INITIAL_MEAN_REWARD = 0.0\n",
    "env.reset()\n",
    "while True:\n",
    "    _, reward, done, _ = env.step(env.action_space.sample())\n",
    "    INITIAL_MEAN_REWARD += reward\n",
    "    if done:\n",
    "        break\n",
    "INITIAL_MEAN_REWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJQNx-7JhrWw"
   },
   "source": [
    "Zu Beginn legen wir die Episodenanzahl fest, die der Agent zum Training durchlaufen soll.\n",
    "Die verschiedenen Listen werden angelegt, um Parameter abzuspeichern, an denen das Training abschließend bewertet werden kann.\n",
    "\n",
    "STEPS uns SYNC beziehen sich auf die Synchronisation des Deep Q-Networks mit dem Target-Network.\n",
    "\n",
    "Wir starten den Trainingsprozess mit einer for-Schleife für EPISODES viele Iterationen. Zu Beginn jeder Iteration wird das Spiel neu gestartet, da eine Episode pro Iteration gespielt wird. Die \"done\"-Flag wird auf False gesetzt, da wir uns nicht länger in einem Endzustand befinden.\n",
    "\n",
    "Die while-Schleife leitet dann eine Episode ein und wird so lange durchlaufen, bis die \"done\"-Flag das Ende einer Episode signalisiert.\n",
    "\n",
    "Zu Beginn jeder Episode wählen wir eine Aktion auf Grundlage des Initialzustandes (state = env.reset()). Während der Episode werden die Aktionen anschließend anhand des aktuellen Episodenzustandes gewählt (next_state, ... = env.step(action)).\n",
    "Durch das Interagieren mit der Umgebung erhält der Agent eine neue Beobachtung (next_state), eine Belohnung (reward), eine Flag (done) und ein info-Dictionary (_) von der Umgebung. Das info-Dictionary enhält weitere Informationen, wie beispielsweise die Leben des Agenten, falls vorhanden. Das Dictionary wird nicht für das Training des Agenten benötigt.\n",
    "\n",
    "Die erhaltenen Parameter werden mit save_transition() in Listen abgespeichert und später für das Training verwendet.\n",
    "Befinden sich ausreichend Daten im MEMORY_BUFFER wird das Training mit replay() gestartet.\n",
    "\n",
    "Zum Schluss wird EPSILON verringert, sodass die Wahrscheinlichkeiten für gute Aktionen erhöht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BujPrlhAhqhc",
    "outputId": "3724df14-34a2-4edd-a2fb-b5b99fe533bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 15_000\n",
    "REWARD_LIST = []\n",
    "MEAN_LIST = []\n",
    "BEST_MEAN_REWARD = INITIAL_MEAN_REWARD\n",
    "EPSILON_LIST = []\n",
    "STEPS = 0\n",
    "SYNC = 1000 #10000\n",
    "\n",
    "# for-Schleife des Trainingsprozesses\n",
    "for episode in range(EPISODES):\n",
    "    EPISODE_REWARD = 0.0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # while-Schleife einer Episode\n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \"\"\" Transition im MEMORY BUFFER speichern \"\"\"\n",
    "        save_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        \"\"\" trainieren des Netzes, falls genügend Transitionen gespeichert \"\"\"\n",
    "        if len(MEMORY_BUFFER) > TRAIN_START:\n",
    "            replay()\n",
    "            \n",
    "        \"\"\" Synchronisation zwischen des Target Networks und Deep Q-Network \"\"\"\n",
    "        if STEPS % SYNC == 0:\n",
    "            TARGET.set_weights(DQN.get_weights())\n",
    "\n",
    "        \"\"\" Reward einer Aktion zum gesamten Reward der Episode addieren \"\"\"\n",
    "        EPISODE_REWARD += reward\n",
    "        \n",
    "        \"\"\" State aktualisieren \"\"\"\n",
    "        state = next_state\n",
    "        STEPS += 1\n",
    "        \n",
    "        \"\"\" EPSILON verringern \"\"\"\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON = epsilon_decay(\"exponential\", EPSILON, EPSILON_DECAY, EPSILON_MIN, STEPS)\n",
    "\n",
    "        if done:\n",
    "            \"\"\" Speichern des Rewards \"\"\"\n",
    "            REWARD_LIST.append(EPISODE_REWARD)\n",
    "            current_mean_reward = np.mean(REWARD_LIST[-min(len(REWARD_LIST), 10):])\n",
    "            MEAN_LIST.append(np.mean(REWARD_LIST))\n",
    "            EPSILON_LIST.append(EPSILON)\n",
    "            \n",
    "            \"\"\" Ausgabe des aktuellen Trainingsfortschrittes \"\"\"\n",
    "            print(\"Episode:\", episode+1, \"\\tReward:\", EPISODE_REWARD, \"\\tMean:\", round(current_mean_reward, 2),\"\\tBestMean:\", BEST_MEAN_REWARD, \"\\tTRAIN START:\", (len(MEMORY_BUFFER)>TRAIN_START), \"\\tEpsi:\", EPSILON)\n",
    "\n",
    "            \"\"\" Übernahme des höchsten Mean Rewards \"\"\"\n",
    "            if current_mean_reward > BEST_MEAN_REWARD:\n",
    "                BEST_MEAN_REWARD = current_mean_reward\n",
    "        \n",
    "                \"\"\" Trainierte Gewichte speichern \"\"\"\n",
    "                if EPSILON < 0.3:\n",
    "                    import os\n",
    "                    try:\n",
    "                        os.makedirs(PATH)\n",
    "                    except FileExistsError:\n",
    "                        # Pfad existiert bereits\n",
    "                        pass\n",
    "                    DQN.save_weights(PATH +\"Best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "date = datetime.now().date()\n",
    "\n",
    "\"\"\" In Listen gespeicherte Daten als CSV auf der Festplatte abspeichern \"\"\"\n",
    "\n",
    "df = pd.DataFrame(list(zip(REWARD_LIST, MEAN_LIST, EPSILON_LIST)), \n",
    "               columns =['Rewards', 'Mean Reward', \"Epsilon\"]) \n",
    "df.to_csv(PATH + game + \"_\" + str(date) + \"_\"+ MODE + \".csv\", mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.save_weights(PATH +\"End.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXt-2mKVOFBw"
   },
   "source": [
    "# **Auswertung des Trainings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "id": "sICat9voOJKZ",
    "outputId": "7cf16408-586a-4c56-bd10-a33044d9d385",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Erstellen einer Grafik über den Trainingsverlauf \"\"\"\n",
    "plt.figure(figsize=(25, 12))\n",
    "plt.plot(REWARD_LIST, label=\"erhaltene Rewards\")\n",
    "plt.plot(MEAN_LIST, label=\"durchschnittler Reward\")\n",
    "plt.title(\"Rewards während des Trainings\", fontsize=25)\n",
    "plt.xlabel(\"Episoden\", fontsize=20)\n",
    "plt.ylabel(\"Rewards\", fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCZYOdYaPGPP"
   },
   "source": [
    "# **Trainierten Agenten spielen lassen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n02s15IFPNVl"
   },
   "outputs": [],
   "source": [
    "# Gewichte laden\n",
    "#WEIGHTS_PATH = \"WEIGHTS/Pong-v0_DQN_Ep_14840.h5\"\n",
    "#DQN.load_weights(filepath=WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdkuM1BjXkah"
   },
   "source": [
    "## Rendering a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "RkdjLEfgUPcf",
    "outputId": "3fd6bf95-1fc3-422a-820e-ab7fb8187b5f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = np.argmax(DQN.predict(state))\n",
    "        state, reward, done, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPQlMVu90wgzQaUbux1p9Bf",
   "collapsed_sections": [],
   "name": "Deep_Q-Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
